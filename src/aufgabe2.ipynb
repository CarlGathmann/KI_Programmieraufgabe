{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0400fe7-1e50-481d-bb7e-62f2c126d4c2",
   "metadata": {},
   "source": [
    "# Aufgabe 2 - Tiefe Einblicke ins Institut"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e59687f3-c461-4f6e-a898-d023cdc8c67c",
   "metadata": {},
   "source": [
    "Den Code immer nachvollziehbar kommentieren! Bitte beachtet, dass das Notebook von Anfang bis Ende ohne Fehler durchlaufen muss und dass die requirements.txt Datei aktualisiert wird."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec5f0020-55eb-4023-a883-90ac6206cfb3",
   "metadata": {},
   "source": [
    "## Teilaufgabe a): Trainings-und Testdatenset\n",
    "## Teilaufgabe b): Verteilung RGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c91f492-e05c-4a7f-9a1c-8674e2ed71d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "def thumbnail(img):\n",
    "    \"\"\"\n",
    "    Resizes the image from 224x224 to 30x30\n",
    "    Image gets reshaped to a 1 dimensional list\n",
    "    :param img: the image\n",
    "    :return: resized image as 1 dimensional list\n",
    "    \"\"\"\n",
    "    img_list = np.asarray(img.resize((30, 30))).reshape(1, 2700)\n",
    "    final_list = np.array([item for sublist in img_list for item in sublist])\n",
    "    return final_list\n",
    "\n",
    "\n",
    "class PicData:\n",
    "    def __init__(self, name):\n",
    "        file_paths = {\n",
    "            'prof': \"Professorenbuero\",\n",
    "            'hall': \"Flur\",\n",
    "            'lab': \"Labor\",\n",
    "            'tea': \"Teekueche\"\n",
    "        }\n",
    "        name_to_label = {\n",
    "            'prof': 1,\n",
    "            'hall': 2,\n",
    "            'lab': 3,\n",
    "            'tea': 4\n",
    "        }\n",
    "        pic_amount = len(os.listdir(f\"Bilder/{file_paths[name]}\"))\n",
    "        self.path = file_paths[name]\n",
    "        # self.pics contains all pictures of one room\n",
    "        self.pics = [Image.open(f\"Bilder/{self.path}/{self.path + str(i + 1)}.jpg\") for i in range(pic_amount)]\n",
    "        # self.data contains all histograms of one room\n",
    "        self.data = np.array([img.histogram() for img in self.pics])\n",
    "        random.shuffle(self.data)\n",
    "        #self.labels = np.array([name_to_label[name] for _ in range(len(self.data))])\n",
    "        #self.img_train, self.img_test, self.lbl_train, self.lbl_test = train_test_split(self.data, self.labels, test_size=0.2)\n",
    "\n",
    "        # creating data tuple: (data, label)\n",
    "        # splitting data into train and test data. 80% training data, 20% test data\n",
    "        self.train = [(self.data[i], name_to_label[name]) for i in range(int(0.8 * len(self.data)))]\n",
    "        self.test  = [(self.data[i], name_to_label[name]) for i in range(int(0.8 * len(self.data)), len(self.data))]\n",
    "\n",
    "# lists contain (data, label) tuple for each picture of each room\n",
    "prof_pics = PicData('prof')\n",
    "hall_pics = PicData('hall')\n",
    "lab_pics = PicData('lab')\n",
    "tea_pics = PicData('tea')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bb17fba-193f-4873-a631-7a902bb54675",
   "metadata": {},
   "source": [
    "## Teilaufgabe c): Training und Test mit Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "a22091366bf1b7e2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# creating a list containing all the training data: [[histogram, label],...]\n",
    "complete_train = list(prof_pics.train) + list(hall_pics.train) + list(lab_pics.train) + list(tea_pics.train)\n",
    "# creating a list containing all the test data: [[histogram, label],...]\n",
    "complete_test = list(prof_pics.test) + list(hall_pics.test) + list(lab_pics.test) + list(tea_pics.test)\n",
    "random.shuffle(complete_train)\n",
    "random.shuffle(complete_test)\n",
    "\n",
    "# splitting list items X_ = histogram, y_ = label\n",
    "X_train = [item[0] for item in complete_train]\n",
    "y_train = [item[1] for item in complete_train]\n",
    "X_test = [item[0] for item in complete_test]\n",
    "y_test = [item[1] for item in complete_test]\n",
    "\n",
    "# train the random forest\n",
    "rf = RandomForestClassifier(n_estimators=155)\n",
    "rf.fit(X_train, y_train)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3615fa0f3c4e7194"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, recall_score, f1_score, precision_score\n",
    "\n",
    "def score (y_expect, y_pred):\n",
    "    \"\"\"\n",
    "    gives information about the random forests stats.\n",
    "    As print out: Forest Predictions, Expectations, Accuracy, F1 Score, Recall and Precision.\n",
    "    As plot: Confusion matrix\n",
    "    :param y_expect: Forest Predictions\n",
    "    :param y_pred: Forest Expectations\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    errors = 0\n",
    "    for i in range(len(y_pred)):\n",
    "        if y_pred[i] != y_expect[i]:\n",
    "            errors += 1\n",
    "\n",
    "    #conf_matrix = confusion_matrix(y_expect, y_pred)\n",
    "    ConfusionMatrixDisplay.from_predictions(y_expect, y_pred)\n",
    "    accuracy = (len(y_expect)-errors)/len(y_expect) * 100\n",
    "\n",
    "    print('Predictions: ', str(y_pred))\n",
    "    print('Expected: ', str(y_expect))\n",
    "    # print('Mean Absolute Error:', str(round(np.mean(errors), 2)) + '°')\n",
    "    print('Accuracy:', round(accuracy, 2), '%')\n",
    "    print(\"F1 Score:\", f1_score(y_expect, y_pred, average='micro'))\n",
    "    print(\"Recall:\", recall_score(y_expect, y_pred, average='micro'))\n",
    "    print(\"Precision:\", precision_score(y_expect, y_pred, average='micro'))\n",
    "    print('Confusion matrix: ')\n",
    "    # str(conf_matrix))\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "y_pred = rf.predict(X_test)\n",
    "score(y_test, y_pred)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d15fcfc54314d60d"
  },
  {
   "cell_type": "markdown",
   "id": "51e14a88-cf54-40a0-b118-05e243e0b995",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Teilaufgabe d): Hyperparameteroptimierung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65263759-2c02-43f0-b756-c8f8ad77b75e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "# choosing max depth parameter\n",
    "param_grid = {#'n_estimators': [10, 55, 100, 155],\n",
    "              'max_depth': [60, 80, None],\n",
    "              #'max_features': ['sqrt', 'log2', None],\n",
    "              #'min_samples_leaf': [1, 3, 5]\n",
    "              #'min_samples_split': [2, 3, 4]\n",
    "              }\n",
    "rf = RandomForestClassifier()\n",
    "grid_search = GridSearchCV(estimator = rf, param_grid = param_grid, cv = 5, scoring='r2') # verbose =2,  n_jobs = -1\n",
    "grid_search.fit(X_train, y_train)\n",
    "print(\"Best Params: \", grid_search.best_params_) # differs every run, but 80 seems to be the best\n",
    "\n",
    "print(\"Scores of the new forest:\")\n",
    "rf = RandomForestClassifier(max_depth = grid_search.best_params_[\"max_depth\"])\n",
    "rf.fit(X_train, y_train)\n",
    "score(y_test, rf.predict(X_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba0cffb4-b4dd-4df8-b7d5-f2f98c8a698b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Teilaufgabe e): Dimensionalitätsreduktion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a72c1da-07de-4252-a4a9-b373918b1cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Calculate eigenvectors and eigenvalues of the covariance matrix\n",
    "X_train = np.array(X_train)\n",
    "mean_vec = np.mean(X_train, axis=0)\n",
    "cov_mat = np.cov(X_train.T)\n",
    "eig_vals, eig_vecs = np.linalg.eig(cov_mat)\n",
    "# Make a list of (eigenvalue, eigenvector) tuples\n",
    "eig_pairs = [(np.abs(eig_vals[i]), eig_vecs[:, i]) for i in range(len(eig_vals))]\n",
    "\n",
    "# Sort the (eigenvalue, eigenvector) tuples from high to low\n",
    "eig_pairs.sort(key=lambda x: x[0], reverse=True)\n",
    "\n",
    "# Calculate the explained variance from the eigenvalues\n",
    "tot = sum(eig_vals)\n",
    "var_exp = [(i / tot) * 100 for i in sorted(eig_vals, reverse=True)] # Individual explained variance\n",
    "cum_var_exp = np.cumsum(var_exp) # Cumulative explained variance\n",
    "\n",
    "# Find the number of components that explain 90% of the variance\n",
    "n = [n for n,i in enumerate(cum_var_exp) if i > 90][0]\n",
    "\n",
    "pca = PCA(n_components=n)\n",
    "pca.fit(X_train)\n",
    "X_20d = pca.transform(X_train)\n",
    "\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=155)\n",
    "rf = rf.fit(X_20d, y_train)\n",
    "\n",
    "test_20d = pca.transform(X_test)\n",
    "score(y_test, rf.predict(test_20d))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
